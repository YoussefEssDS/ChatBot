{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro: \n",
    "\n",
    "Through this tutorial we will implement a Deep NLP ChatBot using Tensorflow. So without further a do let's get right into it.\n",
    "\n",
    "We'll start by importing the libraries needed for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import re #Helps with data preprocessing, with rgular epressions to be exact\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Please make sure your version of Tensorflow is 1.0.0\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for the training of this ChatBot are taking from: \n",
    "\n",
    "https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
    "\n",
    "This dataset is called Cornell Movie--Dialogs Corpus, and it contains conversations between actors from a large number of movies, so the type of our ChatBot would be a friend-like ChatBot (able to do casual conversations), for more field specific ChatBots we can use other kind of datasets. Anyway, for further informations about the data used you can look at the link above.\n",
    "\n",
    "It's important to know that the dataset used is composed of 2 text files: \"movie_lines.txt\" and \"movie_conversations.txt\". The first contains the lines from different movies in an unorderly fashion, but these lines have IDs, these IDs are used in the second file to identify the lines that correspond to a certain conversation, so the second file works as a way to order the line from the first file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data preprocesing: \n",
    "\n",
    "Generally, this is the longest part of each project, in which we will make the data ready for input into the deep learning model. Luckily the er library is here to carry some load of this phase. Let's begin:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data: We will load both the lines and conversations\n",
    "\n",
    "with open(\"C:/Users/YsfDS/Desktop/data/movie_lines.txt\",encoding='utf-8',errors='ignore') as f1:\n",
    "    lines=f1.read().split('\\n') #304714 lines\n",
    "with open(\"C:/Users/YsfDS/Desktop/data/movie_conversations.txt\",encoding='utf-8',errors='ignore') as f2:\n",
    "    convos=f2.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create a dictionary that maps each line with its ID.\n",
    "id2line={}\n",
    "\n",
    "for line in lines:\n",
    "    spl=line.split(' +++$+++ ')\n",
    "    if len(spl)==5:\n",
    "        id2line[spl[0]]=spl[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now create a list of conversations. (IDs of lines in list)\n",
    "\n",
    "convoli= []\n",
    "\n",
    "for conv in convos[:-1]: #The last row of this list is empty\n",
    "    spl=conv.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(' ','')\n",
    "    convoli.append(spl.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the list of convo ids we will try build two lists one for 'questions' and the other for 'answers'.\n",
    "\n",
    "questions=[]\n",
    "answers=[]\n",
    "\n",
    "for conv in convoli:\n",
    "    k=len(conv)\n",
    "    for i in range(k-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for text cleaning\n",
    "\n",
    "def cleanText(text):\n",
    "    # text to lower case\n",
    "    text=text.lower()\n",
    "    # Now to make it easier for the ChatBot to learn we gonna use re to replace expression like \"i'm\" with \"i am\"\n",
    "    text=re.sub(r\"i'm\",\"i am\",text)\n",
    "    text=re.sub(r\"she's\",\"she is\",text)\n",
    "    text=re.sub(r\"he's\",\"he is\",text)\n",
    "    text=re.sub(r\"it's\",\"it is\",text)\n",
    "    text=re.sub(r\"that's\",\"that is\",text)\n",
    "    text=re.sub(r\"what's\",\"what is\",text)\n",
    "    text=re.sub(r\"where's\",\"where is\",text)\n",
    "    text=re.sub(r\"\\'ve\",\" have\",text)\n",
    "    text=re.sub(r\"\\'ll\",\" will\",text)\n",
    "    text=re.sub(r\"\\'d\",\" would\",text)\n",
    "    text=re.sub(r\"\\'re\",\" are\",text)\n",
    "    text=re.sub(r\"won't\",\"will not\",text)\n",
    "    text=re.sub(r\"can't\",\"cannot\",text)\n",
    "    text=re.sub(r\"n't\",\" not\",text)\n",
    "    text=re.sub(r\"[-()/\\\"#$%^&*()_+@=?<>:;,.!{}'|]\",\"\",text)\n",
    "    text=[word for word in text.split() if word.isalpha()]\n",
    "    text=' '.join(text)\n",
    "    #Do as you can in here the better the cleaning the better the result\n",
    "    return(text)\n",
    "\n",
    "clean_questions=[cleanText(line) for line in questions]\n",
    "clean_answers=[cleanText(line) for line in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now remove qst/ans that are too long or too short.\n",
    "\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if 2 <= len(question.split()) <= 25:\n",
    "        short_questions.append(question)\n",
    "        short_answers.append(clean_answers[i])\n",
    "    i += 1\n",
    "clean_questions = []\n",
    "clean_answers = []\n",
    "i = 0\n",
    "for answer in short_answers:\n",
    "    if 2 <= len(answer.split()) <= 25:\n",
    "        clean_answers.append(answer)\n",
    "        clean_questions.append(short_questions[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to optimize our ChatBot training we will try to remove infrequent words from both questions and answers lists.\n",
    "# So the first step to do that is to generate a dictionnary that maps word to their cardinality within the dataset.\n",
    "\n",
    "wordOccur={}\n",
    "for question in clean_questions:\n",
    "    l=question.split()\n",
    "    for i in range (len(l)) :\n",
    "        if l[i] in wordOccur.keys():\n",
    "            wordOccur[l[i]]+=1\n",
    "        else:\n",
    "            wordOccur[l[i]]=1\n",
    "for answer in clean_answers:\n",
    "    l=answer.split()\n",
    "    for i in range (len(l)) :\n",
    "        if l[i] in wordOccur.keys():\n",
    "            wordOccur[l[i]]+=1\n",
    "        else:\n",
    "            wordOccur[l[i]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second step is to set a threshold for the number of occurence of words that will be used in the training of the model.\n",
    "# Let's create 2 dictionaries that map each word from questions/answers to a unique identifier.\n",
    "\n",
    "threshold=15 #This as of now a hyperparameter of the model, 20 seems reasonable we can either decrease it or increase it based on obtained results.\n",
    "\n",
    "Qwords=[q.split()[i] for q in clean_questions for i in range(len(q.split()))] #Words in the questions.\n",
    "Qwords=list(set(Qwords)) #Remove redundencies\n",
    "Awords=[a.split()[i] for a in clean_answers for i in range(len(a.split()))]   #Words in the answers.\n",
    "Awords=list(set(Awords))\n",
    "\n",
    "questionwordsIDs={}\n",
    "\n",
    "wordID=0\n",
    "for word , count in wordOccur.items():\n",
    "    if (count > threshold and word in Qwords):\n",
    "        questionwordsIDs[word]=wordID\n",
    "        wordID+=1\n",
    "        \n",
    "answerwordsIDs={}\n",
    "        \n",
    "wordID=0\n",
    "for word , count in wordOccur.items():\n",
    "    if (count > threshold and word in Awords):\n",
    "        answerwordsIDs[word]=wordID\n",
    "        wordID+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now add tokens necessary for the SEQ2SEQ model to the dictionary with their unique IDs.\n",
    "\n",
    "tokens=['<PAD>','<EOS>','<OUT>','<SOS>']\n",
    "for token in tokens:\n",
    "    questionwordsIDs[token]=len(questionwordsIDs)+1\n",
    "for token in tokens:\n",
    "    answerwordsIDs[token]=len(answerwordsIDs)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the implmentation of the SEQ2SEQ model we will need the inverse mapping ID--> word for the answer dictionary so let's do that.\n",
    "\n",
    "answerIDs2words={wordID:word for word,wordID in answerwordsIDs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add at the end to clean_answers <EOS>.\n",
    "\n",
    "for i in range (len(clean_answers)):\n",
    "    clean_answers[i]+=' <EOS>'\n",
    "\n",
    "# Now we will translate questions and answers into a set of integers which are their IDs as defined before.\n",
    "\n",
    "codedQuestions=[]\n",
    "i=0\n",
    "for question in clean_questions:\n",
    "    l=question.split()\n",
    "    temp=[]\n",
    "    if len(l)>0:\n",
    "        for word in l:\n",
    "            if (word not in questionwordsIDs.keys()):\n",
    "                temp.append(questionwordsIDs['<OUT>'])\n",
    "            else:\n",
    "                temp.append(questionwordsIDs[word])\n",
    "        codedQuestions.append(temp)\n",
    "        i+=1\n",
    "\n",
    "codedAnswers=[]\n",
    "for answer in clean_answers:\n",
    "    l=answer.split()\n",
    "    temp=[]\n",
    "    if len(l)>0:\n",
    "        for word in l:\n",
    "            if (word not in answerwordsIDs.keys()):\n",
    "                temp.append(answerwordsIDs['<OUT>'])\n",
    "            else:\n",
    "                temp.append(answerwordsIDs[word])\n",
    "        codedAnswers.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So final step,  before getting into modeling and what we will need to do is sorting the questions and answers by length\n",
    "# this helps (speed-up) with the learning process. \n",
    "\n",
    "SortclAns = []\n",
    "SortclQues = []\n",
    "for length in range(1, 25 + 1):\n",
    "    for i in enumerate(codedQuestions):\n",
    "        if len(i[1]) == length:\n",
    "            SortclQues.append(codedQuestions[i[0]])\n",
    "            SortclAns.append(codedAnswers[i[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Building the SEQ2SEQ model:\n",
    "\n",
    "Now we will start using Tensorflow to build the architecture of the model that ww will train in the next phase, so let's get into it.\n",
    "\n",
    "It's important to note that in Tensorflow all variables are tensors, a tensor is a special data structure that is without being mathematically rigorous can be considered as a multidimensional vector, a matrix for example is a rank 2 tensor. These tensor based variables allow a fast computation for deep neural networks, so in order to use this tensor variables we must define them in a Tensorflow placeholder. So the first thing we will do is create placeholders for inputs and targets and also for some hyperparameters. Let's go!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelInputs():\n",
    "    inputs=tf.placeholder(tf.int32,[None,None],name='input') # arguments: type, size(matrix: size of batch + sequence length), name\n",
    "    targets=tf.placeholder(tf.int32,[None,None],name='target')\n",
    "    lr=tf.placeholder(tf.float32,name='learning_rate')\n",
    "    keep_prob=tf.placeholder(tf.float32,name='drop_out_rate') #A hyperparameter that designate the dropout rate, generally it's at 20% (This idea helps prevent overfitting)\n",
    "    return(inputs,targets,lr,keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know a RNN model is composed of two main parts, an encoder part that recieves the input sequence, and a decoder that generates sequentially the output. In Tensorflow the decode needs the targets in a particular form which is composed of two main phases. First we must provide targets by batches (a batch size to specified) and also to ensure every target (answer) of the batch starts with a '< SOS >' tag. So that's the plan of attack for the next step. Let's start.\n",
    "\n",
    "![image info](./EN-DE.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_targets(targets,word2int,batch_size): #word2in is the dictionary that maps words to their ID.\n",
    "    left_side=tf.fill([batch_size,1],word2int['<SOS>'])\n",
    "    preProTar=tf.concat([left_side,targets],axis=1)\n",
    "    return(preProTar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will officially start the architecture of the model. So first the encoder:\n",
    "\n",
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length): #rnn_size is number of input tensors in the encoder/ list of length of sequences of the batch\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size) #create the LSTM\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob) #Creating the the dropout\n",
    "    # Till now u just created the architecture of one cell of the RNN(LSTM). Now to create the encoder cell.\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
    "                                                                    cell_bw = encoder_cell,\n",
    "                                                                    sequence_length = sequence_length,\n",
    "                                                                    inputs = rnn_inputs,\n",
    "                                                                    dtype = tf.float32)\n",
    "    # Making the chatbot as good as we can by using a bidirectional RNN.\n",
    "    return encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will implement the fucntion that does the decoding for the training set then returns the decoding\n",
    "# outputs, we also implemented the attention concept. \n",
    "\n",
    "def decode_trainSet(encoder_state,decoder_cell,decoder_embedded_inputs,sequence_length,decoding_scope,output_function,keep_prob,batch_size): #Embeddings are representations of words in a unique vector of numbers, in our case thery are the inputs for the decoder\n",
    "    attention_states=tf.zeros([batch_size,1,decoder_cell.output_size])\n",
    "    attention_keys,attention_values,attention_score_function,attention_construct_function=tf.contrib.seq2seq.prepare_attention(attention_states,attention_option='bahdanau',num_units=decoder_cell.output_size)\n",
    "    training_decoder_function=tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                           attention_keys,\n",
    "                                                                            attention_values,\n",
    "                                                                            attention_score_function,\n",
    "                                                                            attention_construct_function,\n",
    "                                                                           name='att_dec_train')\n",
    "    decoder_output,_,_=tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,training_decoder_function,decoder_embedded_inputs,sequence_length,decoding_scope)\n",
    "    decoder_output_drop_out=tf.nn.dropout(decoder_output,keep_prob)\n",
    "    return(output_function(decoder_output_drop_out))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              name = \"attn_dec_train\")\n",
    "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                              training_decoder_function,\n",
    "                                                                                                              decoder_embedded_input,\n",
    "                                                                                                              sequence_length,\n",
    "                                                                                                              scope = decoding_scope)\n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "    return output_function(decoder_output_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the decoder intended for the test/validation sets. This is going to be very similar to the last part.\n",
    "\n",
    "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
    "                                                                              encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              decoder_embeddings_matrix,\n",
    "                                                                              sos_id,\n",
    "                                                                              eos_id,\n",
    "                                                                              maximum_length,\n",
    "                                                                              num_words,\n",
    "                                                                              name = \"attn_dec_inf\")\n",
    "    test_predictions,_,_ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                test_decoder_function,\n",
    "                                                                scope = decoding_scope)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now at last we create the decoder\n",
    "\n",
    "def decoder_rnn(decoder_embedded_input,decoder_embeddings_matrix,encoder_state,num_words,sequence_length,rnn_size,num_layers,word2int,keep_prob,batch_size):\n",
    "    \n",
    "    with tf.variable_scope('decoding') as decoding_scope:\n",
    "        lstm=tf.contrib.rnn.BasicLSTMCell(rnn_size) #the following 3 lines are same as decoder\n",
    "        lstm_dropOut=tf.contrib.rnn.DropoutWrapper(lstm,input_keep_prob=keep_prob)\n",
    "        decoder_cell=tf.contrib.rnn.MultiRNNCell([lstm_dropOut]*num_layers)\n",
    "        weights=tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases=tf.zeros_initializer()\n",
    "        output_function=lambda x: tf.contrib.layers.fully_connected(x,\n",
    "                                                                   num_words,\n",
    "                                                                   None,\n",
    "                                                                   scope=decoding_scope,\n",
    "                                                                   weights_initializer=weights,\n",
    "                                                                    biases_initializer=biases)\n",
    "        training_predictions=decode_training_set(encoder_state,decoder_cell,decoder_embedded_input,sequence_length,decoding_scope,output_function,keep_prob,batch_size)\n",
    "        \n",
    "        decoding_scope.reuse_variables()\n",
    "        test_predictions=decode_test_set(encoder_state,decoder_cell,decoder_embeddings_matrix,word2int['<SOS>'],word2int['<EOS>'],sequence_length-1,num_words,decoding_scope,output_function,keep_prob,batch_size)\n",
    "    return(training_predictions,test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the SEQ2SEQ Model.\n",
    "\n",
    "def seq2seq_model(inputs,targets,keep_prob,batch_size,sequence_length,answers_num_words,questions_num_words,encoder_embedding_size,decoder_embedding_size,rnn_size,num_layers,questionwordsIDs):\n",
    "    encoder_embeded_input=tf.contrib.layers.embed_sequence(inputs,\n",
    "                                                          answers_num_words+1,\n",
    "                                                          encoder_embedding_size,\n",
    "                                                          initializer=tf.random_uniform_initializer(1,0))\n",
    "    encoder_state=encoder_rnn(encoder_embeded_input,rnn_size,num_layers,keep_prob,sequence_length)\n",
    "    preprocTargets=preprocess_targets(targets,questionwordsIDs,batch_size)\n",
    "    decoder_embeddings_matrix=tf.Variable(tf.random_uniform([questions_num_words+1,decoder_embedding_size],0,1))\n",
    "    decoder_embedded_input=tf.nn.embedding_lookup(decoder_embeddings_matrix,preprocTargets)\n",
    "    training_predictions,test_predictions=decoder_rnn(decoder_embedded_input,decoder_embeddings_matrix,encoder_state,questions_num_words,sequence_length,rnn_size,num_layers,questionwordsIDs,keep_prob,batch_size)\n",
    "    return(training_predictions,test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Training the SEQ2SEQ model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start by setting the hyperparameters thar will be used during the training. Obviously those are to be tweaked to make\n",
    "#the chatbot be better.\n",
    "\n",
    "epochs=100\n",
    "batch_size=64 #It is adviiced to use a batch size that is a multiple of 2.\n",
    "rnn_size=512\n",
    "num_layers=3\n",
    "encoding_embedding_size=512\n",
    "decoding_embedding_size=512\n",
    "learning_rate=0.01\n",
    "learning_rate_decay=0.9\n",
    "min_learning_rate=0.0001\n",
    "keep_probability=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow uses Dataflow programing which is a pradigm that models a program as a oriented graph, for which nodes are the operations and the  edges represent input and output data, this helps with parallelism which is important in Deep learning computations. So to use it we should first create a dataflow graph then create a session to run parts of the graph.\n",
    "\n",
    "So that's what we will do now defining the session for the training phase.\n",
    "\n",
    "Note: The only difference with a regular Session is that an InteractiveSession installs itself as the default session on construction. The methods Tensor.eval() and Operation.run() will use that session to run ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #reseting the tf default graph which we will use.\n",
    "session= tf.InteractiveSession() # Creating the interactive session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model inputs:\n",
    "\n",
    "inputs,targets,lr,keep_prob = modelInputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the sequence length\n",
    "\n",
    "sequence_length=tf.placeholder_with_default(25,None,name='sequence_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the input shape\n",
    "\n",
    "input_shape=tf.shape(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the exciting stuff, by getting the train/test predictions.\n",
    "\n",
    "training_predictions,test_predictions = seq2seq_model(tf.reverse(inputs,[-1]),\n",
    "                                                      targets,\n",
    "                                                      keep_prob,\n",
    "                                                      batch_size,\n",
    "                                                      sequence_length,\n",
    "                                                      len(questionwordsIDs),\n",
    "                                                      len(answerwordsIDs),\n",
    "                                                      encoding_embedding_size,\n",
    "                                                      decoding_embedding_size,\n",
    "                                                      rnn_size,\n",
    "                                                      num_layers,\n",
    "                                                      questionwordsIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting loss error, optimizer and gradient clipping (Forcing gradient to a min/max values if it breaches the bounds).\n",
    "\n",
    "with tf.name_scope('Optimization'):\n",
    "    \n",
    "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,targets,tf.ones([input_shape[0],sequence_length]))\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients=optimizer.compute_gradients(loss_error)\n",
    "    clipped_gradients=[(tf.clip_by_value(grad_tensor,-5.,5.),grad_var) for grad_tensor,grad_var in gradients if grad_tensor is not None]\n",
    "    oprimizer_gradient_clipping=optimizer.apply_gradients(clipped_gradients)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will apply padding. Which means completing a sentence with n words to reach m>n words using <PAD> tags. \n",
    "# this is important in the sense that questions and answers must have same length.\\\n",
    "\n",
    "def apply_padding(batch,word2int): #word2int dict maps a word to integer\n",
    "    max_seq=max([len(sequence) for sequence in batch])\n",
    "    return([seq+[word2int['<PAD>']]*(max_seq-len(seq)) for seq in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into batches of answer and questions\n",
    "\n",
    "def split_into_batches(questions,answers,batch_size):\n",
    "    Qnum_batch=len(answers)//batch_size\n",
    "    for i in range (Qnum_batch):\n",
    "        start=i * batch_size\n",
    "        Qbatch=questions[start:start+batch_size]\n",
    "        Abatch=answers[start:start+batch_size]\n",
    "        #if(len(Qbatch)==len(Abatch)):\n",
    "        paddedQbatch=np.array(apply_padding(Qbatch,questionwordsIDs))\n",
    "        paddedAbatch=np.array(apply_padding(Abatch,answerwordsIDs))\n",
    "        yield  paddedQbatch,paddedAbatch\n",
    "# Notice that yield is inside the loop\n",
    "# Return sends a specified value back to its caller whereas Yield can produce a sequence of values. \n",
    "# We should use yield when we want to iterate over a sequence, but don’t want to store the entire sequence in memory.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data (Q&A) into train/dev/test sets.\n",
    "\n",
    "train_val_split=int(len(SortclQues)*0.15)\n",
    "\n",
    "training_quest=SortclQues[train_val_split:]\n",
    "training_answ=SortclAns[train_val_split:]\n",
    "\n",
    "validation_quest=SortclQues[0:train_val_split]\n",
    "validation_answ=SortclAns[0:train_val_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/100, Batch:    0/2698, Training Loss Error:  0.090, Training Time on 100 Batches: 142 seconds\n",
      "Epoch:   1/100, Batch:  100/2698, Training Loss Error:  6.106, Training Time on 100 Batches: 381 seconds\n",
      "Epoch:   1/100, Batch:  200/2698, Training Loss Error:  4.487, Training Time on 100 Batches: 387 seconds\n",
      "Epoch:   1/100, Batch:  300/2698, Training Loss Error:  4.503, Training Time on 100 Batches: 434 seconds\n",
      "Epoch:   1/100, Batch:  400/2698, Training Loss Error:  4.618, Training Time on 100 Batches: 526 seconds\n",
      "Epoch:   1/100, Batch:  500/2698, Training Loss Error:  4.539, Training Time on 100 Batches: 534 seconds\n",
      "Epoch:   1/100, Batch:  600/2698, Training Loss Error:  4.648, Training Time on 100 Batches: 576 seconds\n",
      "Epoch:   1/100, Batch:  700/2698, Training Loss Error:  4.774, Training Time on 100 Batches: 678 seconds\n",
      "Epoch:   1/100, Batch:  800/2698, Training Loss Error:  4.771, Training Time on 100 Batches: 698 seconds\n",
      "Epoch:   1/100, Batch:  900/2698, Training Loss Error:  4.975, Training Time on 100 Batches: 823 seconds\n",
      "Epoch:   1/100, Batch: 1000/2698, Training Loss Error:  4.957, Training Time on 100 Batches: 709 seconds\n",
      "Epoch:   1/100, Batch: 1100/2698, Training Loss Error:  4.933, Training Time on 100 Batches: 879 seconds\n",
      "Epoch:   1/100, Batch: 1200/2698, Training Loss Error:  5.206, Training Time on 100 Batches: 785 seconds\n",
      "Epoch:   1/100, Batch: 1300/2698, Training Loss Error:  5.058, Training Time on 100 Batches: 932 seconds\n",
      "Validation Loss Error:  3.636, Batch Validation Time: 144 seconds\n",
      "I speak better now!!\n",
      "Epoch:   1/100, Batch: 1400/2698, Training Loss Error:  5.319, Training Time on 100 Batches: 898 seconds\n",
      "Epoch:   1/100, Batch: 1500/2698, Training Loss Error:  5.242, Training Time on 100 Batches: 1170 seconds\n",
      "Epoch:   1/100, Batch: 1600/2698, Training Loss Error:  5.326, Training Time on 100 Batches: 1254 seconds\n",
      "Epoch:   1/100, Batch: 1700/2698, Training Loss Error:  5.449, Training Time on 100 Batches: 1432 seconds\n",
      "Epoch:   1/100, Batch: 1800/2698, Training Loss Error:  5.474, Training Time on 100 Batches: 1224 seconds\n",
      "Epoch:   1/100, Batch: 1900/2698, Training Loss Error:  5.567, Training Time on 100 Batches: 1615 seconds\n",
      "Epoch:   1/100, Batch: 2000/2698, Training Loss Error:  5.553, Training Time on 100 Batches: 1665 seconds\n",
      "Epoch:   1/100, Batch: 2100/2698, Training Loss Error:  5.606, Training Time on 100 Batches: 1970 seconds\n",
      "Epoch:   1/100, Batch: 2200/2698, Training Loss Error:  5.638, Training Time on 100 Batches: 2045 seconds\n",
      "Epoch:   1/100, Batch: 2300/2698, Training Loss Error:  5.684, Training Time on 100 Batches: 2141 seconds\n",
      "Epoch:   1/100, Batch: 2400/2698, Training Loss Error:  5.755, Training Time on 100 Batches: 2624 seconds\n",
      "Epoch:   1/100, Batch: 2500/2698, Training Loss Error:  5.756, Training Time on 100 Batches: 2882 seconds\n",
      "Epoch:   1/100, Batch: 2600/2698, Training Loss Error:  5.759, Training Time on 100 Batches: 2951 seconds\n",
      "Epoch:   2/100, Batch:    0/2698, Training Loss Error:  2.701, Training Time on 100 Batches: 274 seconds\n",
      "Epoch:   2/100, Batch:  100/2698, Training Loss Error:  4.808, Training Time on 100 Batches: 414 seconds\n",
      "Epoch:   2/100, Batch:  200/2698, Training Loss Error:  4.409, Training Time on 100 Batches: 540 seconds\n",
      "Epoch:   2/100, Batch:  300/2698, Training Loss Error:  4.498, Training Time on 100 Batches: 615 seconds\n",
      "Epoch:   2/100, Batch:  400/2698, Training Loss Error:  4.581, Training Time on 100 Batches: 618 seconds\n",
      "Epoch:   2/100, Batch:  500/2698, Training Loss Error:  4.526, Training Time on 100 Batches: 648 seconds\n",
      "Epoch:   2/100, Batch:  600/2698, Training Loss Error:  4.638, Training Time on 100 Batches: 798 seconds\n",
      "Epoch:   2/100, Batch:  700/2698, Training Loss Error:  4.750, Training Time on 100 Batches: 790 seconds\n",
      "Epoch:   2/100, Batch:  800/2698, Training Loss Error:  4.750, Training Time on 100 Batches: 690 seconds\n",
      "Epoch:   2/100, Batch:  900/2698, Training Loss Error:  4.973, Training Time on 100 Batches: 938 seconds\n",
      "Epoch:   2/100, Batch: 1000/2698, Training Loss Error:  4.916, Training Time on 100 Batches: 781 seconds\n",
      "Epoch:   2/100, Batch: 1100/2698, Training Loss Error:  4.941, Training Time on 100 Batches: 979 seconds\n",
      "Epoch:   2/100, Batch: 1200/2698, Training Loss Error:  5.199, Training Time on 100 Batches: 937 seconds\n",
      "Epoch:   2/100, Batch: 1300/2698, Training Loss Error:  5.040, Training Time on 100 Batches: 1020 seconds\n",
      "Validation Loss Error:  3.985, Batch Validation Time: 147 seconds\n",
      "Sorry I do not speak better, I need to practice more.\n",
      "Epoch:   2/100, Batch: 1400/2698, Training Loss Error:  5.296, Training Time on 100 Batches: 1046 seconds\n",
      "Epoch:   2/100, Batch: 1500/2698, Training Loss Error:  5.256, Training Time on 100 Batches: 1074 seconds\n",
      "Epoch:   2/100, Batch: 1600/2698, Training Loss Error:  5.295, Training Time on 100 Batches: 1156 seconds\n",
      "Epoch:   2/100, Batch: 1700/2698, Training Loss Error:  5.438, Training Time on 100 Batches: 1553 seconds\n",
      "Epoch:   2/100, Batch: 1800/2698, Training Loss Error:  5.447, Training Time on 100 Batches: 1626 seconds\n",
      "Epoch:   2/100, Batch: 1900/2698, Training Loss Error:  5.505, Training Time on 100 Batches: 1765 seconds\n",
      "Epoch:   2/100, Batch: 2000/2698, Training Loss Error:  5.543, Training Time on 100 Batches: 1796 seconds\n",
      "Epoch:   2/100, Batch: 2100/2698, Training Loss Error:  5.598, Training Time on 100 Batches: 2084 seconds\n",
      "Epoch:   2/100, Batch: 2200/2698, Training Loss Error:  5.642, Training Time on 100 Batches: 1863 seconds\n",
      "Epoch:   2/100, Batch: 2300/2698, Training Loss Error:  5.673, Training Time on 100 Batches: 2438 seconds\n",
      "Epoch:   2/100, Batch: 2400/2698, Training Loss Error:  5.737, Training Time on 100 Batches: 2296 seconds\n",
      "Epoch:   2/100, Batch: 2500/2698, Training Loss Error:  5.733, Training Time on 100 Batches: 3051 seconds\n",
      "Epoch:   2/100, Batch: 2600/2698, Training Loss Error:  5.754, Training Time on 100 Batches: 2909 seconds\n",
      "Epoch:   3/100, Batch:    0/2698, Training Loss Error:  2.705, Training Time on 100 Batches: 303 seconds\n",
      "Epoch:   3/100, Batch:  100/2698, Training Loss Error:  4.703, Training Time on 100 Batches: 484 seconds\n",
      "Epoch:   3/100, Batch:  200/2698, Training Loss Error:  4.434, Training Time on 100 Batches: 385 seconds\n",
      "Epoch:   3/100, Batch:  300/2698, Training Loss Error:  4.497, Training Time on 100 Batches: 567 seconds\n",
      "Epoch:   3/100, Batch:  400/2698, Training Loss Error:  4.574, Training Time on 100 Batches: 684 seconds\n",
      "Epoch:   3/100, Batch:  500/2698, Training Loss Error:  4.494, Training Time on 100 Batches: 537 seconds\n",
      "Epoch:   3/100, Batch:  600/2698, Training Loss Error:  4.614, Training Time on 100 Batches: 804 seconds\n",
      "Epoch:   3/100, Batch:  700/2698, Training Loss Error:  4.741, Training Time on 100 Batches: 665 seconds\n",
      "Epoch:   3/100, Batch:  800/2698, Training Loss Error:  4.734, Training Time on 100 Batches: 828 seconds\n",
      "Epoch:   3/100, Batch:  900/2698, Training Loss Error:  4.942, Training Time on 100 Batches: 848 seconds\n",
      "Epoch:   3/100, Batch: 1000/2698, Training Loss Error:  4.914, Training Time on 100 Batches: 1006 seconds\n",
      "Epoch:   3/100, Batch: 1100/2698, Training Loss Error:  4.906, Training Time on 100 Batches: 868 seconds\n",
      "Epoch:   3/100, Batch: 1200/2698, Training Loss Error:  5.148, Training Time on 100 Batches: 1137 seconds\n",
      "Epoch:   3/100, Batch: 1300/2698, Training Loss Error:  5.038, Training Time on 100 Batches: 1026 seconds\n",
      "Validation Loss Error:  5.166, Batch Validation Time: 147 seconds\n",
      "Sorry I do not speak better, I need to practice more.\n",
      "Epoch:   3/100, Batch: 1400/2698, Training Loss Error:  5.246, Training Time on 100 Batches: 1438 seconds\n",
      "Epoch:   3/100, Batch: 1500/2698, Training Loss Error:  5.227, Training Time on 100 Batches: 1451 seconds\n",
      "Epoch:   3/100, Batch: 1600/2698, Training Loss Error:  5.288, Training Time on 100 Batches: 1590 seconds\n",
      "Epoch:   3/100, Batch: 1700/2698, Training Loss Error:  5.403, Training Time on 100 Batches: 1732 seconds\n",
      "Epoch:   3/100, Batch: 1800/2698, Training Loss Error:  5.411, Training Time on 100 Batches: 1518 seconds\n",
      "Epoch:   3/100, Batch: 1900/2698, Training Loss Error:  5.472, Training Time on 100 Batches: 1963 seconds\n",
      "Epoch:   3/100, Batch: 2000/2698, Training Loss Error:  5.521, Training Time on 100 Batches: 2112 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   3/100, Batch: 2100/2698, Training Loss Error:  5.578, Training Time on 100 Batches: 1854 seconds\n",
      "Epoch:   3/100, Batch: 2200/2698, Training Loss Error:  5.607, Training Time on 100 Batches: 2474 seconds\n",
      "Epoch:   3/100, Batch: 2300/2698, Training Loss Error:  5.650, Training Time on 100 Batches: 2334 seconds\n",
      "Epoch:   3/100, Batch: 2400/2698, Training Loss Error:  5.689, Training Time on 100 Batches: 3065 seconds\n",
      "Epoch:   3/100, Batch: 2500/2698, Training Loss Error:  5.717, Training Time on 100 Batches: 3388 seconds\n",
      "Epoch:   3/100, Batch: 2600/2698, Training Loss Error:  5.741, Training Time on 100 Batches: 3954 seconds\n",
      "Epoch:   4/100, Batch:    0/2698, Training Loss Error:  2.701, Training Time on 100 Batches: 328 seconds\n",
      "Epoch:   4/100, Batch:  100/2698, Training Loss Error:  4.502, Training Time on 100 Batches: 437 seconds\n",
      "Epoch:   4/100, Batch:  200/2698, Training Loss Error:  4.401, Training Time on 100 Batches: 565 seconds\n",
      "Epoch:   4/100, Batch:  300/2698, Training Loss Error:  4.484, Training Time on 100 Batches: 751 seconds\n",
      "Epoch:   4/100, Batch:  400/2698, Training Loss Error:  4.546, Training Time on 100 Batches: 585 seconds\n",
      "Epoch:   4/100, Batch:  500/2698, Training Loss Error:  4.486, Training Time on 100 Batches: 707 seconds\n",
      "Epoch:   4/100, Batch:  600/2698, Training Loss Error:  4.589, Training Time on 100 Batches: 882 seconds\n",
      "Epoch:   4/100, Batch:  700/2698, Training Loss Error:  4.733, Training Time on 100 Batches: 898 seconds\n",
      "Epoch:   4/100, Batch:  800/2698, Training Loss Error:  4.723, Training Time on 100 Batches: 895 seconds\n",
      "Epoch:   4/100, Batch:  900/2698, Training Loss Error:  4.940, Training Time on 100 Batches: 1117 seconds\n",
      "Epoch:   4/100, Batch: 1000/2698, Training Loss Error:  4.896, Training Time on 100 Batches: 885 seconds\n",
      "Epoch:   4/100, Batch: 1100/2698, Training Loss Error:  4.904, Training Time on 100 Batches: 1167 seconds\n",
      "Epoch:   4/100, Batch: 1200/2698, Training Loss Error:  5.128, Training Time on 100 Batches: 1299 seconds\n",
      "Epoch:   4/100, Batch: 1300/2698, Training Loss Error:  5.020, Training Time on 100 Batches: 1298 seconds\n",
      "Validation Loss Error:  5.545, Batch Validation Time: 142 seconds\n",
      "Sorry I do not speak better, I need to practice more.\n",
      "Epoch:   4/100, Batch: 1400/2698, Training Loss Error:  5.239, Training Time on 100 Batches: 1518 seconds\n",
      "Epoch:   4/100, Batch: 1500/2698, Training Loss Error:  5.203, Training Time on 100 Batches: 1267 seconds\n",
      "Epoch:   4/100, Batch: 1600/2698, Training Loss Error:  5.287, Training Time on 100 Batches: 1663 seconds\n",
      "Epoch:   4/100, Batch: 1700/2698, Training Loss Error:  5.393, Training Time on 100 Batches: 1704 seconds\n",
      "Epoch:   4/100, Batch: 1800/2698, Training Loss Error:  5.386, Training Time on 100 Batches: 1531 seconds\n",
      "Epoch:   4/100, Batch: 1900/2698, Training Loss Error:  5.454, Training Time on 100 Batches: 2077 seconds\n",
      "Epoch:   4/100, Batch: 2000/2698, Training Loss Error:  5.516, Training Time on 100 Batches: 2238 seconds\n",
      "Epoch:   4/100, Batch: 2100/2698, Training Loss Error:  5.585, Training Time on 100 Batches: 1928 seconds\n",
      "Epoch:   4/100, Batch: 2200/2698, Training Loss Error:  5.616, Training Time on 100 Batches: 2223 seconds\n",
      "Epoch:   4/100, Batch: 2300/2698, Training Loss Error:  5.645, Training Time on 100 Batches: 2445 seconds\n",
      "Epoch:   4/100, Batch: 2400/2698, Training Loss Error:  5.672, Training Time on 100 Batches: 2667 seconds\n",
      "Epoch:   4/100, Batch: 2500/2698, Training Loss Error:  5.707, Training Time on 100 Batches: 3142 seconds\n",
      "Epoch:   4/100, Batch: 2600/2698, Training Loss Error:  5.732, Training Time on 100 Batches: 4134 seconds\n",
      "Epoch:   5/100, Batch:    0/2698, Training Loss Error:  2.700, Training Time on 100 Batches: 334 seconds\n",
      "Epoch:   5/100, Batch:  100/2698, Training Loss Error:  4.466, Training Time on 100 Batches: 568 seconds\n",
      "Epoch:   5/100, Batch:  200/2698, Training Loss Error:  4.392, Training Time on 100 Batches: 596 seconds\n",
      "Epoch:   5/100, Batch:  300/2698, Training Loss Error:  4.448, Training Time on 100 Batches: 771 seconds\n",
      "Epoch:   5/100, Batch:  400/2698, Training Loss Error:  4.542, Training Time on 100 Batches: 637 seconds\n",
      "Epoch:   5/100, Batch:  500/2698, Training Loss Error:  4.475, Training Time on 100 Batches: 592 seconds\n",
      "Epoch:   5/100, Batch:  600/2698, Training Loss Error:  4.581, Training Time on 100 Batches: 773 seconds\n",
      "Epoch:   5/100, Batch:  700/2698, Training Loss Error:  4.716, Training Time on 100 Batches: 782 seconds\n",
      "Epoch:   5/100, Batch:  800/2698, Training Loss Error:  4.713, Training Time on 100 Batches: 931 seconds\n",
      "Epoch:   5/100, Batch:  900/2698, Training Loss Error:  4.989, Training Time on 100 Batches: 1137 seconds\n",
      "Epoch:   5/100, Batch: 1000/2698, Training Loss Error:  4.893, Training Time on 100 Batches: 938 seconds\n",
      "Epoch:   5/100, Batch: 1100/2698, Training Loss Error:  4.900, Training Time on 100 Batches: 990 seconds\n",
      "Epoch:   5/100, Batch: 1200/2698, Training Loss Error:  5.148, Training Time on 100 Batches: 1095 seconds\n",
      "Epoch:   5/100, Batch: 1300/2698, Training Loss Error:  5.032, Training Time on 100 Batches: 1335 seconds\n",
      "Validation Loss Error:  4.740, Batch Validation Time: 145 seconds\n",
      "Sorry I do not speak better, I need to practice more.\n",
      "Epoch:   5/100, Batch: 1400/2698, Training Loss Error:  5.231, Training Time on 100 Batches: 1278 seconds\n",
      "Epoch:   5/100, Batch: 1500/2698, Training Loss Error:  5.187, Training Time on 100 Batches: 1304 seconds\n",
      "Epoch:   5/100, Batch: 1600/2698, Training Loss Error:  5.280, Training Time on 100 Batches: 1393 seconds\n",
      "Epoch:   5/100, Batch: 1700/2698, Training Loss Error:  5.367, Training Time on 100 Batches: 1631 seconds\n",
      "Epoch:   5/100, Batch: 1800/2698, Training Loss Error:  5.372, Training Time on 100 Batches: 1999 seconds\n",
      "Epoch:   5/100, Batch: 1900/2698, Training Loss Error:  5.460, Training Time on 100 Batches: 2185 seconds\n",
      "Epoch:   5/100, Batch: 2000/2698, Training Loss Error:  5.494, Training Time on 100 Batches: 2360 seconds\n",
      "Epoch:   5/100, Batch: 2100/2698, Training Loss Error:  5.548, Training Time on 100 Batches: 2431 seconds\n",
      "Epoch:   5/100, Batch: 2200/2698, Training Loss Error:  5.586, Training Time on 100 Batches: 2204 seconds\n",
      "Epoch:   5/100, Batch: 2300/2698, Training Loss Error:  5.637, Training Time on 100 Batches: 2515 seconds\n",
      "Epoch:   5/100, Batch: 2400/2698, Training Loss Error:  5.662, Training Time on 100 Batches: 2649 seconds\n",
      "Epoch:   5/100, Batch: 2500/2698, Training Loss Error:  5.706, Training Time on 100 Batches: 3723 seconds\n",
      "Epoch:   5/100, Batch: 2600/2698, Training Loss Error:  5.727, Training Time on 100 Batches: 4321 seconds\n",
      "Epoch:   6/100, Batch:    0/2698, Training Loss Error:  2.697, Training Time on 100 Batches: 342 seconds\n",
      "Epoch:   6/100, Batch:  100/2698, Training Loss Error:  4.441, Training Time on 100 Batches: 564 seconds\n",
      "Epoch:   6/100, Batch:  200/2698, Training Loss Error:  4.384, Training Time on 100 Batches: 581 seconds\n",
      "Epoch:   6/100, Batch:  300/2698, Training Loss Error:  4.436, Training Time on 100 Batches: 612 seconds\n",
      "Epoch:   6/100, Batch:  400/2698, Training Loss Error:  4.520, Training Time on 100 Batches: 735 seconds\n",
      "Epoch:   6/100, Batch:  500/2698, Training Loss Error:  4.469, Training Time on 100 Batches: 803 seconds\n",
      "Epoch:   6/100, Batch:  600/2698, Training Loss Error:  4.573, Training Time on 100 Batches: 973 seconds\n",
      "Epoch:   6/100, Batch:  700/2698, Training Loss Error:  4.715, Training Time on 100 Batches: 1004 seconds\n",
      "Epoch:   6/100, Batch:  800/2698, Training Loss Error:  4.707, Training Time on 100 Batches: 848 seconds\n",
      "Epoch:   6/100, Batch:  900/2698, Training Loss Error:  4.951, Training Time on 100 Batches: 951 seconds\n",
      "Epoch:   6/100, Batch: 1000/2698, Training Loss Error:  4.890, Training Time on 100 Batches: 1206 seconds\n",
      "Epoch:   6/100, Batch: 1100/2698, Training Loss Error:  4.894, Training Time on 100 Batches: 1290 seconds\n",
      "Epoch:   6/100, Batch: 1200/2698, Training Loss Error:  5.137, Training Time on 100 Batches: 1451 seconds\n",
      "Epoch:   6/100, Batch: 1300/2698, Training Loss Error:  5.024, Training Time on 100 Batches: 1454 seconds\n",
      "Validation Loss Error:  4.746, Batch Validation Time: 148 seconds\n",
      "Sorry I do not speak better, I need to practice more.\n",
      "Epoch:   6/100, Batch: 1400/2698, Training Loss Error:  5.224, Training Time on 100 Batches: 1567 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   6/100, Batch: 1500/2698, Training Loss Error:  5.250, Training Time on 100 Batches: 1403 seconds\n",
      "Epoch:   6/100, Batch: 1600/2698, Training Loss Error:  5.384, Training Time on 100 Batches: 1606 seconds\n",
      "Epoch:   6/100, Batch: 1700/2698, Training Loss Error:  5.367, Training Time on 100 Batches: 2001 seconds\n",
      "Epoch:   6/100, Batch: 1800/2698, Training Loss Error:  5.372, Training Time on 100 Batches: 1998 seconds\n",
      "Epoch:   6/100, Batch: 1900/2698, Training Loss Error:  5.451, Training Time on 100 Batches: 2282 seconds\n",
      "Epoch:   6/100, Batch: 2000/2698, Training Loss Error:  5.491, Training Time on 100 Batches: 2477 seconds\n",
      "Epoch:   6/100, Batch: 2100/2698, Training Loss Error:  5.549, Training Time on 100 Batches: 2182 seconds\n",
      "Epoch:   6/100, Batch: 2200/2698, Training Loss Error:  5.588, Training Time on 100 Batches: 2695 seconds\n",
      "Epoch:   6/100, Batch: 2300/2698, Training Loss Error:  5.636, Training Time on 100 Batches: 2870 seconds\n",
      "Epoch:   6/100, Batch: 2400/2698, Training Loss Error:  5.663, Training Time on 100 Batches: 2821 seconds\n",
      "Epoch:   6/100, Batch: 2500/2698, Training Loss Error:  5.701, Training Time on 100 Batches: 3568 seconds\n",
      "Epoch:   6/100, Batch: 2600/2698, Training Loss Error:  5.721, Training Time on 100 Batches: 4255 seconds\n",
      "Epoch:   7/100, Batch:    0/2698, Training Loss Error:  2.697, Training Time on 100 Batches: 384 seconds\n",
      "Epoch:   7/100, Batch:  100/2698, Training Loss Error:  4.401, Training Time on 100 Batches: 548 seconds\n",
      "Epoch:   7/100, Batch:  200/2698, Training Loss Error:  4.369, Training Time on 100 Batches: 623 seconds\n",
      "Epoch:   7/100, Batch:  300/2698, Training Loss Error:  4.438, Training Time on 100 Batches: 849 seconds\n",
      "Epoch:   7/100, Batch:  400/2698, Training Loss Error:  4.547, Training Time on 100 Batches: 682 seconds\n",
      "Epoch:   7/100, Batch:  500/2698, Training Loss Error:  4.463, Training Time on 100 Batches: 864 seconds\n",
      "Epoch:   7/100, Batch:  600/2698, Training Loss Error:  4.571, Training Time on 100 Batches: 1045 seconds\n",
      "Epoch:   7/100, Batch:  700/2698, Training Loss Error:  4.710, Training Time on 100 Batches: 1054 seconds\n",
      "Epoch:   7/100, Batch:  800/2698, Training Loss Error:  4.696, Training Time on 100 Batches: 1056 seconds\n",
      "Epoch:   7/100, Batch:  900/2698, Training Loss Error:  4.957, Training Time on 100 Batches: 1267 seconds\n",
      "Epoch:   7/100, Batch: 1000/2698, Training Loss Error:  4.891, Training Time on 100 Batches: 1156 seconds\n",
      "Epoch:   7/100, Batch: 1100/2698, Training Loss Error:  4.889, Training Time on 100 Batches: 1078 seconds\n",
      "Epoch:   7/100, Batch: 1200/2698, Training Loss Error:  5.138, Training Time on 100 Batches: 1507 seconds\n",
      "Epoch:   7/100, Batch: 1300/2698, Training Loss Error:  5.019, Training Time on 100 Batches: 1234 seconds\n",
      "Validation Loss Error:  4.811, Batch Validation Time: 152 seconds\n",
      "Sorry I do not speak better, I need to practice more.\n",
      "Epoch:   7/100, Batch: 1400/2698, Training Loss Error:  5.221, Training Time on 100 Batches: 1743 seconds\n",
      "Epoch:   7/100, Batch: 1500/2698, Training Loss Error:  5.185, Training Time on 100 Batches: 1485 seconds\n",
      "Epoch:   7/100, Batch: 1600/2698, Training Loss Error:  5.270, Training Time on 100 Batches: 1887 seconds\n",
      "Epoch:   7/100, Batch: 1700/2698, Training Loss Error:  5.362, Training Time on 100 Batches: 2034 seconds\n",
      "Epoch:   7/100, Batch: 1800/2698, Training Loss Error:  5.371, Training Time on 100 Batches: 2132 seconds\n",
      "Epoch:   7/100, Batch: 1900/2698, Training Loss Error:  5.450, Training Time on 100 Batches: 2088 seconds\n",
      "Epoch:   7/100, Batch: 2000/2698, Training Loss Error:  5.487, Training Time on 100 Batches: 2554 seconds\n",
      "Epoch:   7/100, Batch: 2100/2698, Training Loss Error:  5.549, Training Time on 100 Batches: 2234 seconds\n",
      "Epoch:   7/100, Batch: 2200/2698, Training Loss Error:  5.587, Training Time on 100 Batches: 2434 seconds\n",
      "Epoch:   7/100, Batch: 2300/2698, Training Loss Error:  5.637, Training Time on 100 Batches: 3101 seconds\n",
      "Epoch:   7/100, Batch: 2400/2698, Training Loss Error:  5.662, Training Time on 100 Batches: 3566 seconds\n",
      "Epoch:   7/100, Batch: 2500/2698, Training Loss Error:  5.702, Training Time on 100 Batches: 3282 seconds\n",
      "Epoch:   7/100, Batch: 2600/2698, Training Loss Error:  5.722, Training Time on 100 Batches: 3782 seconds\n",
      "Epoch:   8/100, Batch:    0/2698, Training Loss Error:  2.697, Training Time on 100 Batches: 406 seconds\n",
      "Epoch:   8/100, Batch:  100/2698, Training Loss Error:  4.464, Training Time on 100 Batches: 654 seconds\n",
      "Epoch:   8/100, Batch:  200/2698, Training Loss Error:  4.377, Training Time on 100 Batches: 657 seconds\n",
      "Epoch:   8/100, Batch:  300/2698, Training Loss Error:  4.442, Training Time on 100 Batches: 746 seconds\n",
      "Epoch:   8/100, Batch:  400/2698, Training Loss Error:  4.542, Training Time on 100 Batches: 729 seconds\n",
      "Epoch:   8/100, Batch:  500/2698, Training Loss Error:  4.469, Training Time on 100 Batches: 885 seconds\n",
      "Epoch:   8/100, Batch:  600/2698, Training Loss Error:  4.574, Training Time on 100 Batches: 1038 seconds\n",
      "Epoch:   8/100, Batch:  700/2698, Training Loss Error:  4.720, Training Time on 100 Batches: 1010 seconds\n",
      "Epoch:   8/100, Batch:  800/2698, Training Loss Error:  4.698, Training Time on 100 Batches: 1095 seconds\n",
      "Epoch:   8/100, Batch:  900/2698, Training Loss Error:  4.956, Training Time on 100 Batches: 1263 seconds\n",
      "Epoch:   8/100, Batch: 1000/2698, Training Loss Error:  4.884, Training Time on 100 Batches: 1337 seconds\n",
      "Epoch:   8/100, Batch: 1100/2698, Training Loss Error:  4.916, Training Time on 100 Batches: 1090 seconds\n",
      "Epoch:   8/100, Batch: 1200/2698, Training Loss Error:  5.162, Training Time on 100 Batches: 1559 seconds\n",
      "Epoch:   8/100, Batch: 1300/2698, Training Loss Error:  5.012, Training Time on 100 Batches: 1568 seconds\n",
      "Validation Loss Error:  4.242, Batch Validation Time: 149 seconds\n",
      "Sorry I do not speak better, I need to practice more.\n",
      "Epoch:   8/100, Batch: 1400/2698, Training Loss Error:  5.222, Training Time on 100 Batches: 1624 seconds\n",
      "Epoch:   8/100, Batch: 1500/2698, Training Loss Error:  5.185, Training Time on 100 Batches: 1927 seconds\n",
      "Epoch:   8/100, Batch: 1600/2698, Training Loss Error:  5.269, Training Time on 100 Batches: 1954 seconds\n",
      "Epoch:   8/100, Batch: 1700/2698, Training Loss Error:  5.371, Training Time on 100 Batches: 1720 seconds\n",
      "Epoch:   8/100, Batch: 1800/2698, Training Loss Error:  5.369, Training Time on 100 Batches: 2137 seconds\n",
      "Epoch:   8/100, Batch: 1900/2698, Training Loss Error:  5.449, Training Time on 100 Batches: 2396 seconds\n",
      "Epoch:   8/100, Batch: 2000/2698, Training Loss Error:  5.481, Training Time on 100 Batches: 2260 seconds\n",
      "Epoch:   8/100, Batch: 2100/2698, Training Loss Error:  5.549, Training Time on 100 Batches: 2488 seconds\n",
      "Epoch:   8/100, Batch: 2200/2698, Training Loss Error:  5.581, Training Time on 100 Batches: 3046 seconds\n",
      "Epoch:   8/100, Batch: 2300/2698, Training Loss Error:  5.639, Training Time on 100 Batches: 3393 seconds\n",
      "Epoch:   8/100, Batch: 2400/2698, Training Loss Error:  5.675, Training Time on 100 Batches: 3099 seconds\n",
      "Epoch:   8/100, Batch: 2500/2698, Training Loss Error:  5.701, Training Time on 100 Batches: 4016 seconds\n",
      "Epoch:   8/100, Batch: 2600/2698, Training Loss Error:  5.717, Training Time on 100 Batches: 3882 seconds\n",
      "Epoch:   9/100, Batch:    0/2698, Training Loss Error:  2.693, Training Time on 100 Batches: 421 seconds\n",
      "Epoch:   9/100, Batch:  100/2698, Training Loss Error:  4.473, Training Time on 100 Batches: 540 seconds\n",
      "Epoch:   9/100, Batch:  200/2698, Training Loss Error:  4.379, Training Time on 100 Batches: 701 seconds\n",
      "Epoch:   9/100, Batch:  300/2698, Training Loss Error:  4.451, Training Time on 100 Batches: 809 seconds\n",
      "Epoch:   9/100, Batch:  400/2698, Training Loss Error:  4.552, Training Time on 100 Batches: 745 seconds\n",
      "Epoch:   9/100, Batch:  500/2698, Training Loss Error:  4.469, Training Time on 100 Batches: 764 seconds\n",
      "Epoch:   9/100, Batch:  600/2698, Training Loss Error:  4.579, Training Time on 100 Batches: 1120 seconds\n",
      "Epoch:   9/100, Batch:  700/2698, Training Loss Error:  4.720, Training Time on 100 Batches: 1143 seconds\n",
      "Epoch:   9/100, Batch:  800/2698, Training Loss Error:  4.700, Training Time on 100 Batches: 1148 seconds\n",
      "Epoch:   9/100, Batch:  900/2698, Training Loss Error:  4.930, Training Time on 100 Batches: 1376 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   9/100, Batch: 1000/2698, Training Loss Error:  4.881, Training Time on 100 Batches: 1370 seconds\n",
      "Epoch:   9/100, Batch: 1100/2698, Training Loss Error:  4.885, Training Time on 100 Batches: 1204 seconds\n",
      "Epoch:   9/100, Batch: 1200/2698, Training Loss Error:  5.112, Training Time on 100 Batches: 1640 seconds\n",
      "Epoch:   9/100, Batch: 1300/2698, Training Loss Error:  5.009, Training Time on 100 Batches: 1320 seconds\n",
      "Validation Loss Error:  4.169, Batch Validation Time: 155 seconds\n",
      "Sorry I do not speak better, I need to practice more.\n",
      "Epoch:   9/100, Batch: 1400/2698, Training Loss Error:  5.194, Training Time on 100 Batches: 1846 seconds\n",
      "Epoch:   9/100, Batch: 1500/2698, Training Loss Error:  5.165, Training Time on 100 Batches: 1557 seconds\n",
      "Epoch:   9/100, Batch: 1600/2698, Training Loss Error:  5.262, Training Time on 100 Batches: 1985 seconds\n",
      "Epoch:   9/100, Batch: 1700/2698, Training Loss Error:  5.357, Training Time on 100 Batches: 1746 seconds\n",
      "Epoch:   9/100, Batch: 1800/2698, Training Loss Error:  5.366, Training Time on 100 Batches: 2252 seconds\n",
      "Epoch:   9/100, Batch: 1900/2698, Training Loss Error:  5.445, Training Time on 100 Batches: 2110 seconds\n",
      "Epoch:   9/100, Batch: 2000/2698, Training Loss Error:  5.478, Training Time on 100 Batches: 2651 seconds\n",
      "Epoch:   9/100, Batch: 2100/2698, Training Loss Error:  5.544, Training Time on 100 Batches: 2390 seconds\n",
      "Epoch:   9/100, Batch: 2200/2698, Training Loss Error:  5.580, Training Time on 100 Batches: 2588 seconds\n",
      "Epoch:   9/100, Batch: 2300/2698, Training Loss Error:  5.633, Training Time on 100 Batches: 2765 seconds\n",
      "Epoch:   9/100, Batch: 2400/2698, Training Loss Error:  5.669, Training Time on 100 Batches: 3841 seconds\n",
      "Epoch:   9/100, Batch: 2500/2698, Training Loss Error:  5.701, Training Time on 100 Batches: 4162 seconds\n",
      "Epoch:   9/100, Batch: 2600/2698, Training Loss Error:  5.717, Training Time on 100 Batches: 3912 seconds\n",
      "Epoch:  10/100, Batch:    0/2698, Training Loss Error:  2.692, Training Time on 100 Batches: 418 seconds\n",
      "Epoch:  10/100, Batch:  100/2698, Training Loss Error:  4.465, Training Time on 100 Batches: 678 seconds\n",
      "Epoch:  10/100, Batch:  200/2698, Training Loss Error:  4.382, Training Time on 100 Batches: 568 seconds\n",
      "Epoch:  10/100, Batch:  300/2698, Training Loss Error:  4.456, Training Time on 100 Batches: 860 seconds\n",
      "Epoch:  10/100, Batch:  400/2698, Training Loss Error:  4.542, Training Time on 100 Batches: 757 seconds\n",
      "Epoch:  10/100, Batch:  500/2698, Training Loss Error:  4.463, Training Time on 100 Batches: 935 seconds\n",
      "Epoch:  10/100, Batch:  600/2698, Training Loss Error:  4.573, Training Time on 100 Batches: 959 seconds\n",
      "Epoch:  10/100, Batch:  700/2698, Training Loss Error:  4.716, Training Time on 100 Batches: 965 seconds\n",
      "Epoch:  10/100, Batch:  800/2698, Training Loss Error:  4.697, Training Time on 100 Batches: 928 seconds\n",
      "Epoch:  10/100, Batch:  900/2698, Training Loss Error:  4.935, Training Time on 100 Batches: 1399 seconds\n",
      "Epoch:  10/100, Batch: 1000/2698, Training Loss Error:  4.881, Training Time on 100 Batches: 1359 seconds\n",
      "Epoch:  10/100, Batch: 1100/2698, Training Loss Error:  4.881, Training Time on 100 Batches: 1203 seconds\n",
      "Epoch:  10/100, Batch: 1200/2698, Training Loss Error:  5.106, Training Time on 100 Batches: 1640 seconds\n",
      "Epoch:  10/100, Batch: 1300/2698, Training Loss Error:  5.007, Training Time on 100 Batches: 1368 seconds\n",
      "Validation Loss Error:  6.728, Batch Validation Time: 153 seconds\n",
      "Sorry I do not speak better, I need to practice more.\n",
      "Epoch:  10/100, Batch: 1400/2698, Training Loss Error:  5.195, Training Time on 100 Batches: 1851 seconds\n",
      "Epoch:  10/100, Batch: 1500/2698, Training Loss Error:  5.165, Training Time on 100 Batches: 1909 seconds\n",
      "Epoch:  10/100, Batch: 1600/2698, Training Loss Error:  5.260, Training Time on 100 Batches: 1618 seconds\n",
      "Epoch:  10/100, Batch: 1700/2698, Training Loss Error:  5.346, Training Time on 100 Batches: 1773 seconds\n",
      "Epoch:  10/100, Batch: 1800/2698, Training Loss Error:  5.358, Training Time on 100 Batches: 1860 seconds\n",
      "Epoch:  10/100, Batch: 1900/2698, Training Loss Error:  5.443, Training Time on 100 Batches: 2484 seconds\n",
      "Epoch:  10/100, Batch: 2000/2698, Training Loss Error:  5.474, Training Time on 100 Batches: 2698 seconds\n",
      "Epoch:  10/100, Batch: 2100/2698, Training Loss Error:  5.541, Training Time on 100 Batches: 2362 seconds\n",
      "Epoch:  10/100, Batch: 2200/2698, Training Loss Error:  5.579, Training Time on 100 Batches: 2712 seconds\n",
      "Epoch:  10/100, Batch: 2300/2698, Training Loss Error:  5.630, Training Time on 100 Batches: 3449 seconds\n",
      "Epoch:  10/100, Batch: 2400/2698, Training Loss Error:  5.669, Training Time on 100 Batches: 3755 seconds\n",
      "Epoch:  10/100, Batch: 2500/2698, Training Loss Error:  5.700, Training Time on 100 Batches: 4065 seconds\n",
      "Epoch:  10/100, Batch: 2600/2698, Training Loss Error:  5.713, Training Time on 100 Batches: 4527 seconds\n",
      "Epoch:  11/100, Batch:    0/2698, Training Loss Error:  2.696, Training Time on 100 Batches: 415 seconds\n",
      "Epoch:  11/100, Batch:  100/2698, Training Loss Error:  4.443, Training Time on 100 Batches: 695 seconds\n",
      "Epoch:  11/100, Batch:  200/2698, Training Loss Error:  4.377, Training Time on 100 Batches: 707 seconds\n",
      "Epoch:  11/100, Batch:  300/2698, Training Loss Error:  4.443, Training Time on 100 Batches: 978 seconds\n",
      "Epoch:  11/100, Batch:  400/2698, Training Loss Error:  4.544, Training Time on 100 Batches: 815 seconds\n",
      "Epoch:  11/100, Batch:  500/2698, Training Loss Error:  4.467, Training Time on 100 Batches: 785 seconds\n",
      "Epoch:  11/100, Batch:  600/2698, Training Loss Error:  4.573, Training Time on 100 Batches: 1156 seconds\n",
      "Epoch:  11/100, Batch:  700/2698, Training Loss Error:  4.714, Training Time on 100 Batches: 1167 seconds\n",
      "Epoch:  11/100, Batch:  800/2698, Training Loss Error:  4.696, Training Time on 100 Batches: 1148 seconds\n",
      "Epoch:  11/100, Batch:  900/2698, Training Loss Error:  4.935, Training Time on 100 Batches: 1404 seconds\n",
      "Epoch:  11/100, Batch: 1000/2698, Training Loss Error:  4.881, Training Time on 100 Batches: 1123 seconds\n",
      "Epoch:  11/100, Batch: 1100/2698, Training Loss Error:  4.881, Training Time on 100 Batches: 1207 seconds\n",
      "Epoch:  11/100, Batch: 1200/2698, Training Loss Error:  5.105, Training Time on 100 Batches: 1473 seconds\n",
      "Epoch:  11/100, Batch: 1300/2698, Training Loss Error:  5.008, Training Time on 100 Batches: 1512 seconds\n",
      "Validation Loss Error:  6.719, Batch Validation Time: 155 seconds\n",
      "Sorry I do not speak better, I need to practice more.\n",
      "Epoch:  11/100, Batch: 1400/2698, Training Loss Error:  5.194, Training Time on 100 Batches: 1860 seconds\n",
      "Epoch:  11/100, Batch: 1500/2698, Training Loss Error:  5.164, Training Time on 100 Batches: 1643 seconds\n",
      "Epoch:  11/100, Batch: 1600/2698, Training Loss Error:  5.256, Training Time on 100 Batches: 2002 seconds\n",
      "Epoch:  11/100, Batch: 1700/2698, Training Loss Error:  5.346, Training Time on 100 Batches: 2195 seconds\n",
      "Epoch:  11/100, Batch: 1800/2698, Training Loss Error:  5.355, Training Time on 100 Batches: 2288 seconds\n",
      "Epoch:  11/100, Batch: 1900/2698, Training Loss Error:  5.442, Training Time on 100 Batches: 2051 seconds\n",
      "Epoch:  11/100, Batch: 2000/2698, Training Loss Error:  5.469, Training Time on 100 Batches: 2292 seconds\n",
      "Epoch:  11/100, Batch: 2100/2698, Training Loss Error:  5.543, Training Time on 100 Batches: 2916 seconds\n",
      "Epoch:  11/100, Batch: 2200/2698, Training Loss Error:  5.580, Training Time on 100 Batches: 2960 seconds\n",
      "Epoch:  11/100, Batch: 2300/2698, Training Loss Error:  5.629, Training Time on 100 Batches: 3465 seconds\n",
      "Epoch:  11/100, Batch: 2400/2698, Training Loss Error:  5.666, Training Time on 100 Batches: 3754 seconds\n",
      "Epoch:  11/100, Batch: 2500/2698, Training Loss Error:  5.700, Training Time on 100 Batches: 4009 seconds\n",
      "Epoch:  11/100, Batch: 2600/2698, Training Loss Error:  5.714, Training Time on 100 Batches: 4824 seconds\n",
      "Epoch:  12/100, Batch:    0/2698, Training Loss Error:  2.696, Training Time on 100 Batches: 354 seconds\n",
      "Epoch:  12/100, Batch:  100/2698, Training Loss Error:  4.443, Training Time on 100 Batches: 585 seconds\n",
      "Epoch:  12/100, Batch:  200/2698, Training Loss Error:  4.372, Training Time on 100 Batches: 709 seconds\n",
      "Epoch:  12/100, Batch:  300/2698, Training Loss Error:  4.447, Training Time on 100 Batches: 990 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12/100, Batch:  400/2698, Training Loss Error:  4.545, Training Time on 100 Batches: 809 seconds\n",
      "Epoch:  12/100, Batch:  500/2698, Training Loss Error:  4.460, Training Time on 100 Batches: 921 seconds\n",
      "Epoch:  12/100, Batch:  600/2698, Training Loss Error:  4.575, Training Time on 100 Batches: 1076 seconds\n",
      "Epoch:  12/100, Batch:  700/2698, Training Loss Error:  4.717, Training Time on 100 Batches: 971 seconds\n",
      "Epoch:  12/100, Batch:  800/2698, Training Loss Error:  4.698, Training Time on 100 Batches: 943 seconds\n",
      "Epoch:  12/100, Batch:  900/2698, Training Loss Error:  4.932, Training Time on 100 Batches: 1315 seconds\n",
      "Epoch:  12/100, Batch: 1000/2698, Training Loss Error:  4.876, Training Time on 100 Batches: 1413 seconds\n",
      "Epoch:  12/100, Batch: 1100/2698, Training Loss Error:  4.881, Training Time on 100 Batches: 1512 seconds\n",
      "Epoch:  12/100, Batch: 1200/2698, Training Loss Error:  5.108, Training Time on 100 Batches: 1335 seconds\n"
     ]
    }
   ],
   "source": [
    "# Training:\n",
    "\n",
    "batch_index_check_training_loss = 100\n",
    "batch_index_check_validation_loss = ((len(training_quest)) // batch_size // 2) - 1\n",
    "total_training_loss_error = 0\n",
    "list_validation_loss_error = []\n",
    "early_stopping_check = 0\n",
    "early_stopping_stop = 1000\n",
    "checkpoint = \"./chatbot_weights.ckpt\" \n",
    "session.run(tf.global_variables_initializer())\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_quest, training_answ, batch_size)):\n",
    "        starting_time = time.time()\n",
    "        _, batch_training_loss_error = session.run([oprimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n",
    "                                                                                               targets: padded_answers_in_batch,\n",
    "                                                                                               lr: learning_rate,\n",
    "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                                                keep_prob:0.5})\n",
    "        total_training_loss_error += batch_training_loss_error\n",
    "        ending_time = time.time()\n",
    "        batch_time = ending_time - starting_time\n",
    "        if batch_index % batch_index_check_training_loss == 0:\n",
    "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
    "                                                                                                                                       epochs,\n",
    "                                                                                                                                       batch_index,\n",
    "                                                                                                                                       len(training_quest) // batch_size,\n",
    "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
    "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
    "            \n",
    "            total_training_loss_error = 0\n",
    "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
    "            total_validation_loss_error = 0\n",
    "            starting_time = time.time()\n",
    "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_quest, validation_answ, batch_size)):\n",
    "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
    "                                                                       targets: padded_answers_in_batch,\n",
    "                                                                       lr: learning_rate,\n",
    "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                       keep_prob: 1})\n",
    "                total_validation_loss_error += batch_validation_loss_error\n",
    "            ending_time = time.time()\n",
    "            batch_time = ending_time - starting_time\n",
    "            average_validation_loss_error = total_validation_loss_error / (len(validation_quest) / batch_size)\n",
    "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
    "            #implementing learning rate decay.\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            # Now for early stopping:\n",
    "            list_validation_loss_error.append(average_validation_loss_error)\n",
    "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
    "                print('I speak better now!!') # Meaning we improved validation loss error. It is smaller than before.\n",
    "                early_stopping_check = 0\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session, checkpoint) #we defined checkpoint before\n",
    "            else:\n",
    "                print(\"Sorry I do not speak better, I need to practice more.\")\n",
    "                early_stopping_check += 1\n",
    "                if early_stopping_check == early_stopping_stop:\n",
    "                    break\n",
    "    if early_stopping_check == early_stopping_stop:\n",
    "        print(\"Sorry! I can't speak better anymore, this is my limit!\")\n",
    "        break\n",
    "print(\"Game Over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
