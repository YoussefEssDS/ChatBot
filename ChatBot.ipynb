{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro: \n",
    "\n",
    "Through this tutorial we will implement a Deep NLP ChatBot using Tensorflow. So without further a do let's get right into it.\n",
    "\n",
    "We'll start by importing the libraries needed for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import re #Helps with data preprocessing\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for the training of this ChatBot are taking from: \n",
    "\n",
    "https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
    "\n",
    "This dataset is called Cornell Movie--Dialogs Corpus, and it contains conversations between actors from a large number of movies, so the type of our ChatBot would be a friend-like ChatBot (able to do casual conversations), for more field specific ChatBots we can use other kind of datasets. Anyway, for further informations about the data used you can look at the link above.\n",
    "\n",
    "It's important to know that the dataset used is composed of 2 text files: \"movie_lines.txt\" and \"movie_conversations.txt\". The first contains the lines from different movies in an unorderly fashion, but these lines have IDs, these IDs are used in the second file to identify the lines that correspond to a certain conversation, so the second file works as a way to order the line fro first file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data preprocesing: \n",
    "\n",
    "Generally, this is the longest part of each project, in which we will make the data ready for input into the deep learning model. Luckily the er library is here to carry some load of this phase. Let's begin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data: We will load both the lines and conversations\n",
    "\n",
    "with open(\"C:/Users/YsfEss/Desktop/data/movie_lines.txt\",encoding='utf-8',errors='ignore') as f1:\n",
    "    lines=f1.read().split('\\n') #304714 lines\n",
    "with open(\"C:/Users/YsfEss/Desktop/data/movie_conversations.txt\",encoding='utf-8',errors='ignore') as f2:\n",
    "    convos=f2.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create a dictionary that maps each line with its ID.\n",
    "id2line={}\n",
    "\n",
    "for line in lines:\n",
    "    spl=line.split(' +++$+++ ')\n",
    "    if len(spl)==5:\n",
    "        id2line[spl[0]]=spl[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now create a list of conversations. \n",
    "\n",
    "convoli= []\n",
    "\n",
    "for conv in convos[:-1]: #The last row of this list is empty\n",
    "    spl=conv.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(' ','')\n",
    "    convoli.append(spl.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the list of convo ids we will try build two lists one for 'questions' and the other for 'answers'.\n",
    "\n",
    "questions=[]\n",
    "answers=[]\n",
    "\n",
    "for conv in convoli:\n",
    "    k=len(conv)\n",
    "    for i in range(k-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for text cleaning\n",
    "\n",
    "def cleanText(text):\n",
    "    # text to lower case\n",
    "    text=text.lower()\n",
    "    # Now to make it easier for the ChatBot to learn we gonna use re to replace expression like \"i'm\" with \"i am\"\n",
    "    text=re.sub(r\"i'm\",\"i am\",text)\n",
    "    text=re.sub(r\"she's\",\"she is\",text)\n",
    "    text=re.sub(r\"he's\",\"he is\",text)\n",
    "    text=re.sub(r\"it's\",\"it is\",text)\n",
    "    text=re.sub(r\"that's\",\"that is\",text)\n",
    "    text=re.sub(r\"what's\",\"what is\",text)\n",
    "    text=re.sub(r\"where's\",\"where is\",text)\n",
    "    text=re.sub(r\"\\'ve\",\" have\",text)\n",
    "    text=re.sub(r\"\\'ll\",\" will\",text)\n",
    "    text=re.sub(r\"\\'d\",\" would\",text)\n",
    "    text=re.sub(r\"\\'re\",\" are\",text)\n",
    "    text=re.sub(r\"won't\",\"would not\",text)\n",
    "    text=re.sub(r\"can't\",\"can not\",text)\n",
    "    text=re.sub(r\"wouldn't\",\"would not\",text)\n",
    "    text=re.sub(r\"couldn't\",\"could not\",text)\n",
    "    text=re.sub(r\"haven't\",\"have not\",text)\n",
    "    text=re.sub(r\"didn't\",\"did not\",text)\n",
    "    text=re.sub(r\"cannot\",\"can not\",text)\n",
    "    text=re.sub(r\"gonna\",\"going to\",text)\n",
    "    text=re.sub(r\"wanna\",\"want to\",text)\n",
    "    text=re.sub(r\"don't\",\"do not\",text)\n",
    "    text=re.sub(r\"[-()/\\\"#$%^&*()_+@=?<>:;,.!{}'|]\",\"\",text)\n",
    "    #Do as you can in here the better the cleaning the better the result\n",
    "    return(text)\n",
    "\n",
    "clean_questions=[cleanText(line) for line in questions if len(cleanText(line))!=0]\n",
    "clean_answers=[cleanText(line) for line in answers if len(cleanText(line))!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to optimize our ChatBot training we will try to remove infrequent words from both questions and answers lists.\n",
    "# So the first step to do that is to generate a dictionnary that maps word to their cardinality within the dataset.\n",
    "\n",
    "wordOccur={}\n",
    "for question in clean_questions:\n",
    "    l=question.split()\n",
    "    for i in range (len(l)) :\n",
    "        if l[i] in wordOccur.keys():\n",
    "            wordOccur[l[i]]+=1\n",
    "        else:\n",
    "            wordOccur[l[i]]=1\n",
    "for answer in clean_answers:\n",
    "    l=answer.split()\n",
    "    for i in range (len(l)) :\n",
    "        if l[i] in wordOccur.keys():\n",
    "            wordOccur[l[i]]+=1\n",
    "        else:\n",
    "            wordOccur[l[i]]=1\n",
    "            \n",
    "# The second step is to set a threshold for the number of occurence of words that will be used in the training of the model.\n",
    "# Let's create a 2 dictionaries that maps each word from questions/answers to a unique identifier.\n",
    "\n",
    "treshold=20 #This as of now a hyperparameter of the model, 20 seems reasonable we can either decrease it or increase it based on obtained results.\n",
    "\n",
    "Qwords=[q.split()[i] for q in clean_questions for i in range(len(q.split()))] #Words in the questions.\n",
    "Qwords=list(set(Qwords)) #Remove redundencies\n",
    "Awords=[a.split()[i] for a in clean_answers for i in range(len(a.split()))]   #Words in the answers.\n",
    "Awords=list(set(Awords))\n",
    "\n",
    "questionwordsIDs={}\n",
    "\n",
    "wordID=0\n",
    "for word , count in wordOccur.items():\n",
    "    if (count > 20 and word in Qwords):\n",
    "        questionwordsIDs[word]=wordID\n",
    "        wordID+=1\n",
    "        \n",
    "answerwordsIDs={}\n",
    "        \n",
    "wordID=0\n",
    "for word , count in wordOccur.items():\n",
    "    if (count > 20 and word in Awords):\n",
    "        answerwordsIDs[word]=wordID\n",
    "        wordID+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now add tokens necessary for the SEQ2SEQ model to the dictionary with their unique IDs.\n",
    "\n",
    "tokens=['<PAD>','<EOS>','<OUT>','<SOS>']\n",
    "for token in tokens:\n",
    "    questionwordsIDs[token]=len(questionwordsIDs)+1\n",
    "for token in tokens:\n",
    "    answerwordsIDs[token]=len(answerwordsIDs)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In thw implmentation of the SEQ2SEQ model we will need the inverse mapping ID--> word for the answer dictionary so let's do that.\n",
    "\n",
    "answerIDs2words={wordID:word for word,wordID in answerwordsIDs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add at the end to clean_answers <EOS>.\n",
    "\n",
    "for i in range (len(clean_answers)):\n",
    "    clean_answers[i]+=' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will translate questions and answers into a set of integers which are their IDs as defined as before.\n",
    "\n",
    "codedQuestions=[]\n",
    "i=0\n",
    "for question in clean_questions:\n",
    "    l=question.split()\n",
    "    temp=[]\n",
    "    if len(l)>0:\n",
    "        for word in l:\n",
    "            if (word not in questionwordsIDs.keys()):\n",
    "                temp.append(questionwordsIDs['<OUT>'])\n",
    "            else:\n",
    "                temp.append(questionwordsIDs[word])\n",
    "        if len(temp)==0:\n",
    "            print(i)\n",
    "        codedQuestions.append(temp)\n",
    "        i+=1\n",
    "\n",
    "codedAnswers=[]\n",
    "for answer in clean_answers:\n",
    "    l=answer.split()\n",
    "    temp=[]\n",
    "    if len(l)>0:\n",
    "        for word in l:\n",
    "            if (word not in answerwordsIDs.keys()):\n",
    "                temp.append(answerwordsIDs['<OUT>'])\n",
    "            else:\n",
    "                temp.append(answerwordsIDs[word])\n",
    "        codedAnswers.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So final step,  before getting into modeling and what we will need to do is sorting the questions and answers by length\n",
    "# this helps (speed-up) with the learning process. \n",
    "\n",
    "SortclQues=sorted(codedQuestions,key=len)\n",
    "SortclAns=sorted(codedAnswers,key=len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Building the SEQ2SEQ model:\n",
    "\n",
    "Now we will start using Tensorflow to build the architecture of the model that ww will train in the next phase, so let's get into it.\n",
    "\n",
    "It's important to note that in Tensorflow all variables are tensors, a tensor is a special data structure that is without being mathematically rigorous can be considered as a multidimensional vector, a matrix for example is a rank 2 tensor. These tensor based variables allow a fast computation for deep neural networks, so in order to use this tensor variables we must define them in a Tensorflow placeholder. So the first thing we will do is create placeholders for inputs and targets. Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelInputs():\n",
    "    inputs=tf.placeholder()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
