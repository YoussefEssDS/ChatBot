{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro: \n",
    "\n",
    "Through this tutorial we will implement a Deep NLP ChatBot using Tensorflow. So without further a do let's get right into it.\n",
    "\n",
    "We'll start by importing the libraries needed for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import re #Helps with data preprocessing\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Please make sure your version of Tensorflow is 1.0.0\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for the training of this ChatBot are taking from: \n",
    "\n",
    "https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
    "\n",
    "This dataset is called Cornell Movie--Dialogs Corpus, and it contains conversations between actors from a large number of movies, so the type of our ChatBot would be a friend-like ChatBot (able to do casual conversations), for more field specific ChatBots we can use other kind of datasets. Anyway, for further informations about the data used you can look at the link above.\n",
    "\n",
    "It's important to know that the dataset used is composed of 2 text files: \"movie_lines.txt\" and \"movie_conversations.txt\". The first contains the lines from different movies in an unorderly fashion, but these lines have IDs, these IDs are used in the second file to identify the lines that correspond to a certain conversation, so the second file works as a way to order the line fro first file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data preprocesing: \n",
    "\n",
    "Generally, this is the longest part of each project, in which we will make the data ready for input into the deep learning model. Luckily the er library is here to carry some load of this phase. Let's begin:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data: We will load both the lines and conversations\n",
    "\n",
    "with open(\"C:/Users/YsfEss/Desktop/data/movie_lines.txt\",encoding='utf-8',errors='ignore') as f1:\n",
    "    lines=f1.read().split('\\n') #304714 lines\n",
    "with open(\"C:/Users/YsfEss/Desktop/data/movie_conversations.txt\",encoding='utf-8',errors='ignore') as f2:\n",
    "    convos=f2.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create a dictionary that maps each line with its ID.\n",
    "id2line={}\n",
    "\n",
    "for line in lines:\n",
    "    spl=line.split(' +++$+++ ')\n",
    "    if len(spl)==5:\n",
    "        id2line[spl[0]]=spl[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now create a list of conversations. (IDs of lines in list)\n",
    "\n",
    "convoli= []\n",
    "\n",
    "for conv in convos[:-1]: #The last row of this list is empty\n",
    "    spl=conv.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(' ','')\n",
    "    convoli.append(spl.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the list of convo ids we will try build two lists one for 'questions' and the other for 'answers'.\n",
    "\n",
    "questions=[]\n",
    "answers=[]\n",
    "\n",
    "for conv in convoli:\n",
    "    k=len(conv)\n",
    "    for i in range(k-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for text cleaning\n",
    "\n",
    "def cleanText(text):\n",
    "    # text to lower case\n",
    "    text=text.lower()\n",
    "    # Now to make it easier for the ChatBot to learn we gonna use re to replace expression like \"i'm\" with \"i am\"\n",
    "    text=re.sub(r\"i'm\",\"i am\",text)\n",
    "    text=re.sub(r\"she's\",\"she is\",text)\n",
    "    text=re.sub(r\"he's\",\"he is\",text)\n",
    "    text=re.sub(r\"it's\",\"it is\",text)\n",
    "    text=re.sub(r\"that's\",\"that is\",text)\n",
    "    text=re.sub(r\"what's\",\"what is\",text)\n",
    "    text=re.sub(r\"where's\",\"where is\",text)\n",
    "    text=re.sub(r\"\\'ve\",\" have\",text)\n",
    "    text=re.sub(r\"\\'ll\",\" will\",text)\n",
    "    text=re.sub(r\"\\'d\",\" would\",text)\n",
    "    text=re.sub(r\"\\'re\",\" are\",text)\n",
    "    text=re.sub(r\"won't\",\"would not\",text)\n",
    "    text=re.sub(r\"can't\",\"can not\",text)\n",
    "    text=re.sub(r\"wouldn't\",\"would not\",text)\n",
    "    text=re.sub(r\"couldn't\",\"could not\",text)\n",
    "    text=re.sub(r\"haven't\",\"have not\",text)\n",
    "    text=re.sub(r\"didn't\",\"did not\",text)\n",
    "    text=re.sub(r\"cannot\",\"can not\",text)\n",
    "    text=re.sub(r\"gonna\",\"going to\",text)\n",
    "    text=re.sub(r\"wanna\",\"want to\",text)\n",
    "    text=re.sub(r\"don't\",\"do not\",text)\n",
    "    text=re.sub(r\"[-()/\\\"#$%^&*()_+@=?<>:;,.!{}'|]\",\"\",text)\n",
    "    #Do as you can in here the better the cleaning the better the result\n",
    "    return(text)\n",
    "\n",
    "clean_questions=[cleanText(line) for line in questions if len(cleanText(line))!=0]\n",
    "clean_answers=[cleanText(line) for line in answers if len(cleanText(line))!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to optimize our ChatBot training we will try to remove infrequent words from both questions and answers lists.\n",
    "# So the first step to do that is to generate a dictionnary that maps word to their cardinality within the dataset.\n",
    "\n",
    "wordOccur={}\n",
    "for question in clean_questions:\n",
    "    l=question.split()\n",
    "    for i in range (len(l)) :\n",
    "        if l[i] in wordOccur.keys():\n",
    "            wordOccur[l[i]]+=1\n",
    "        else:\n",
    "            wordOccur[l[i]]=1\n",
    "for answer in clean_answers:\n",
    "    l=answer.split()\n",
    "    for i in range (len(l)) :\n",
    "        if l[i] in wordOccur.keys():\n",
    "            wordOccur[l[i]]+=1\n",
    "        else:\n",
    "            wordOccur[l[i]]=1\n",
    "            \n",
    "            \n",
    "# The second step is to set a threshold for the number of occurence of words that will be used in the training of the model.\n",
    "# Let's create a 2 dictionaries that maps each word from questions/answers to a unique identifier.\n",
    "\n",
    "treshold=20 #This as of now a hyperparameter of the model, 20 seems reasonable we can either decrease it or increase it based on obtained results.\n",
    "\n",
    "Qwords=[q.split()[i] for q in clean_questions for i in range(len(q.split()))] #Words in the questions.\n",
    "Qwords=list(set(Qwords)) #Remove redundencies\n",
    "Awords=[a.split()[i] for a in clean_answers for i in range(len(a.split()))]   #Words in the answers.\n",
    "Awords=list(set(Awords))\n",
    "\n",
    "questionwordsIDs={}\n",
    "\n",
    "wordID=0\n",
    "for word , count in wordOccur.items():\n",
    "    if (count > 20 and word in Qwords):\n",
    "        questionwordsIDs[word]=wordID\n",
    "        wordID+=1\n",
    "        \n",
    "answerwordsIDs={}\n",
    "        \n",
    "wordID=0\n",
    "for word , count in wordOccur.items():\n",
    "    if (count > 20 and word in Awords):\n",
    "        answerwordsIDs[word]=wordID\n",
    "        wordID+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now add tokens necessary for the SEQ2SEQ model to the dictionary with their unique IDs.\n",
    "\n",
    "tokens=['<PAD>','<EOS>','<OUT>','<SOS>']\n",
    "for token in tokens:\n",
    "    questionwordsIDs[token]=len(questionwordsIDs)+1\n",
    "for token in tokens:\n",
    "    answerwordsIDs[token]=len(answerwordsIDs)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the implmentation of the SEQ2SEQ model we will need the inverse mapping ID--> word for the answer dictionary so let's do that.\n",
    "\n",
    "answerIDs2words={wordID:word for word,wordID in answerwordsIDs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add at the end to clean_answers <EOS>.\n",
    "\n",
    "for i in range (len(clean_answers)):\n",
    "    clean_answers[i]+=' <EOS>'\n",
    "\n",
    "# Now we will translate questions and answers into a set of integers which are their IDs as defined before.\n",
    "\n",
    "codedQuestions=[]\n",
    "i=0\n",
    "for question in clean_questions:\n",
    "    l=question.split()\n",
    "    temp=[]\n",
    "    if len(l)>0:\n",
    "        for word in l:\n",
    "            if (word not in questionwordsIDs.keys()):\n",
    "                temp.append(questionwordsIDs['<OUT>'])\n",
    "            else:\n",
    "                temp.append(questionwordsIDs[word])\n",
    "        if len(temp)==0:\n",
    "            print(i)\n",
    "        codedQuestions.append(temp)\n",
    "        i+=1\n",
    "\n",
    "codedAnswers=[]\n",
    "for answer in clean_answers:\n",
    "    l=answer.split()\n",
    "    temp=[]\n",
    "    if len(l)>0:\n",
    "        for word in l:\n",
    "            if (word not in answerwordsIDs.keys()):\n",
    "                temp.append(answerwordsIDs['<OUT>'])\n",
    "            else:\n",
    "                temp.append(answerwordsIDs[word])\n",
    "        codedAnswers.append(temp)\n",
    "\n",
    "# So final step,  before getting into modeling and what we will need to do is sorting the questions and answers by length\n",
    "# this helps (speed-up) with the learning process. \n",
    "\n",
    "SortclQues=[x for x in sorted(codedQuestions,key=len) if len(x)<=25]\n",
    "SortclAns=[x for x in sorted(codedAnswers,key=len) if len(x)<=25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Building the SEQ2SEQ model:\n",
    "\n",
    "Now we will start using Tensorflow to build the architecture of the model that ww will train in the next phase, so let's get into it.\n",
    "\n",
    "It's important to note that in Tensorflow all variables are tensors, a tensor is a special data structure that is without being mathematically rigorous can be considered as a multidimensional vector, a matrix for example is a rank 2 tensor. These tensor based variables allow a fast computation for deep neural networks, so in order to use this tensor variables we must define them in a Tensorflow placeholder. So the first thing we will do is create placeholders for inputs and targets and also for some hyperparameters. Let's go!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelInputs():\n",
    "    inputs=tf.placeholder(tf.int32,[None,None],name='input') # arguments: type, size(matrix: size of batch + sequence length), name\n",
    "    targets=tf.placeholder(tf.int32,[None,None],name='target')\n",
    "    lr=tf.placeholder(tf.float32,name='learning_rate')\n",
    "    keep_prob=tf.placeholder(tf.float32,name='drop_out_rate') #A hyperparameter that designate the dropout rate, generally it's at 20% (This idea helps prevent overfitting)\n",
    "    return(inputs,targets,lr,keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know a RNN model is composed of two main parts, an encoder part that recieves the input sequence, and a decoder that generates dequentially the output. In Tensorflow the decode needs the targets in a particular form which is composed of two main phases. First we must provide taargets by batches (a batch size to specified) and also to ensure every target (answer) of the batch starts with a '< SOS >' tag. So that's the plan of attack for the next step. Let's start.\n",
    "\n",
    "![image info](./EN-DE.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_targets(targets,word2int,batch_size): #word2in is the dictionary that maps words to their ID.\n",
    "    left_side=tf.fill([batch_size,1],word2int['<SOS>'])\n",
    "    preProTar=tf.concat([left_side,targets],axis=1)\n",
    "    return(preProTar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will officially start the architecture of the model. So first the encoder:\n",
    "\n",
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length): #rnn_size is number of input tensors in the encoder/ list of length of sequences of the batch\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size) #create the LSTM\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob) #Creating the the dropout\n",
    "    # Till now u just created the architecture of one cell of the RNN(LSTM). Now to create the encoder cell.\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
    "                                                                    cell_bw = encoder_cell,\n",
    "                                                                    sequence_length = sequence_length,\n",
    "                                                                    inputs = rnn_inputs,\n",
    "                                                                    dtype = tf.float32)\n",
    "    # Making the chatbot as good as we can by using a bidirectional RNN.\n",
    "    return encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will implement the fucntion that does the decoding for the training set then returns the decoding\n",
    "# outputs, we also implemented the attention concept. \n",
    "\n",
    "def decode_trainSet(encoder_state,decoder_cell,decoder_embedded_inputs,sequence_length,decoding_scope,output_function,keep_prob,batch_size): #Embeddings are representations of words in a unique vector of numbers, in our case thery are the inputs for the decoder\n",
    "    attention_states=tf.zeros([batch_size,1,decoder_cell.output_size])\n",
    "    attention_keys,attention_values,attention_score_function,attention_construct_function=tf.contrib.seq2seq.prepare_attention(attention_states,attention_option='bahdanau',num_units=decoder_cell.output_size)\n",
    "    training_decoder_function=tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                           attention_keys,\n",
    "                                                                            attention_values,\n",
    "                                                                            attention_score_function,\n",
    "                                                                            attention_construct_function,\n",
    "                                                                           name='att_dec_train')\n",
    "    decoder_output,_,_=tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,training_decoder_function,decoder_embedded_inputs,sequence_length,decoding_scope)\n",
    "    decoder_output_drop_out=tf.nn.dropout(decoder_output,keep_prob)\n",
    "    return(output_function(decoder_output_drop_out))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              name = \"attn_dec_train\")\n",
    "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                              training_decoder_function,\n",
    "                                                                                                              decoder_embedded_input,\n",
    "                                                                                                              sequence_length,\n",
    "                                                                                                              scope = decoding_scope)\n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "    return output_function(decoder_output_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the decoder intended for the test/validation sets. This is going to be very similar to the last part.\n",
    "\n",
    "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
    "                                                                              encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              decoder_embeddings_matrix,\n",
    "                                                                              sos_id,\n",
    "                                                                              eos_id,\n",
    "                                                                              maximum_length,\n",
    "                                                                              num_words,\n",
    "                                                                              name = \"attn_dec_inf\")\n",
    "    test_predictions,_,_ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                test_decoder_function,\n",
    "                                                                scope = decoding_scope)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now at last we create the decoder\n",
    "\n",
    "def decoder_rnn(decoder_embedded_input,decoder_embeddings_matrix,encoder_state,num_words,sequence_length,rnn_size,num_layers,word2int,keep_prob,batch_size):\n",
    "    \n",
    "    with tf.variable_scope('decoding') as decoding_scope:\n",
    "        lstm=tf.contrib.rnn.BasicLSTMCell(rnn_size) #the following 3 lines are same as decoder\n",
    "        lstm_dropOut=tf.contrib.rnn.DropoutWrapper(lstm,input_keep_prob=keep_prob)\n",
    "        decoder_cell=tf.contrib.rnn.MultiRNNCell([lstm_dropOut]*num_layers)\n",
    "        weights=tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases=tf.zeros_initializer()\n",
    "        output_function=lambda x: tf.contrib.layers.fully_connected(x,\n",
    "                                                                   num_words,\n",
    "                                                                   None,\n",
    "                                                                   scope=decoding_scope,\n",
    "                                                                   weights_initializer=weights,\n",
    "                                                                    biases_initializer=biases)\n",
    "        training_predictions=decode_training_set(encoder_state,decoder_cell,decoder_embedded_input,sequence_length,decoding_scope,output_function,keep_prob,batch_size)\n",
    "        \n",
    "        decoding_scope.reuse_variables()\n",
    "        test_predictions=decode_test_set(encoder_state,decoder_cell,decoder_embeddings_matrix,word2int['<SOS>'],word2int['<EOS>'],sequence_length-1,num_words,decoding_scope,output_function,keep_prob,batch_size)\n",
    "    return(training_predictions,test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the SEQ2SEQ Model.\n",
    "\n",
    "def seq2seq_model(inputs,targets,keep_prob,batch_size,sequence_length,answers_num_words,questions_num_words,encoder_embedding_size,decoder_embedding_size,rnn_size,num_layers,questionwordsIDs):\n",
    "    encoder_embeded_input=tf.contrib.layers.embed_sequence(inputs,\n",
    "                                                          answers_num_words+1,\n",
    "                                                          encoder_embedding_size,\n",
    "                                                          initializer=tf.random_uniform_initializer(1,0))\n",
    "    encoder_state=encoder_rnn(encoder_embeded_input,rnn_size,num_layers,keep_prob,sequence_length)\n",
    "    preprocTargets=preprocess_targets(targets,questionwordsIDs,batch_size)\n",
    "    decoder_embeddings_matrix=tf.Variable(tf.random_uniform([questions_num_words+1,decoder_embedding_size],0,1))\n",
    "    decoder_embedded_input=tf.nn.embedding_lookup(decoder_embeddings_matrix,preprocTargets)\n",
    "    training_predictions,test_predictions=decoder_rnn(decoder_embedded_input,decoder_embeddings_matrix,encoder_state,questions_num_words,sequence_length,rnn_size,num_layers,questionwordsIDs,keep_prob,batch_size)\n",
    "    return(training_predictions,test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Training the SEQ2SEQ model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start by setting the hyperparameters thar will be used during the training. Obviously those are to be tweaked to make\n",
    "#the chatbot be better.\n",
    "\n",
    "epochs=100\n",
    "batch_size=64 #It is adviiced to use a batch size that is a multiple of 2.\n",
    "rnn_size=512\n",
    "num_layers=3\n",
    "encoding_embedding_size=512\n",
    "decoding_embedding_size=512\n",
    "learning_rate=0.01\n",
    "learning_rate_decay=0.9\n",
    "min_learning_rate=0.0001\n",
    "keep_probability=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow uses Dataflow programing which is a pradigm that models a program as a oriented graph, for which nodes are the operations and the  edges represent input and output data, this helps with parallelism which is important in Deep learning computations. So to use it we should first create a dataflow graph then create a session to run parts of the graph.\n",
    "\n",
    "So that's what we will do now defining the session for the training phase.\n",
    "\n",
    "Note: The only difference with a regular Session is that an InteractiveSession installs itself as the default session on construction. The methods Tensor.eval() and Operation.run() will use that session to run ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #reseting the tf default graph which we will use.\n",
    "session= tf.InteractiveSession() # Creating the interactive session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model inputs:\n",
    "\n",
    "inputs,targets,lr,keep_prob = modelInputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the sequence length\n",
    "\n",
    "sequence_length=tf.placeholder_with_default(25,None,name='sequence_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the input shape\n",
    "\n",
    "input_shape=tf.shape(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the exciting stuff, by getting the train/test predictions.\n",
    "\n",
    "training_predictions,test_predictions = seq2seq_model(tf.reverse(inputs,[-1]),\n",
    "                                                      targets,\n",
    "                                                      keep_prob,\n",
    "                                                      batch_size,\n",
    "                                                      sequence_length,\n",
    "                                                      len(questionwordsIDs),\n",
    "                                                      len(answerwordsIDs),\n",
    "                                                      encoding_embedding_size,\n",
    "                                                      decoding_embedding_size,\n",
    "                                                      rnn_size,\n",
    "                                                      num_layers,\n",
    "                                                      questionwordsIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting loss error, optimizer and gradient clipping (Forcing gradient to a min/max values if it breaches the bounds).\n",
    "\n",
    "with tf.name_scope('Optimization'):\n",
    "    \n",
    "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,targets,tf.ones([input_shape[0],sequence_length]))\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients=optimizer.compute_gradients(loss_error)\n",
    "    clipped_gradients=[(tf.clip_by_value(grad_tensor,-5.,5.),grad_var) for grad_tensor,grad_var in gradients if grad_tensor is not None]\n",
    "    oprimizer_gradient_clipping=optimizer.apply_gradients(clipped_gradients)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will apply padding. Which means completing a sentence with n words to reach m>n words using <PAD> tags. \n",
    "# this is important in the sense that questions and answers must have same length.\\\n",
    "\n",
    "def apply_padding(batch,word2int): #word2int dict maps a word to integer\n",
    "    max_seq=max([len(sequence) for sequence in batch])\n",
    "    return([seq+[word2int['<PAD>']]*(max_seq-len(seq)) for seq in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into batches of answer and questions\n",
    "\n",
    "def split_into_batches(questions,answers,batch_size):\n",
    "    Qnum_batch=len(questions)//batch_size\n",
    "    for i in range (Qnum_batch):\n",
    "        start=i * batch_size\n",
    "        Qbatch=questions[start:start+batch_size]\n",
    "        Abatch=answers[start:start+batch_size]\n",
    "        if(len(Qbatch)==len(Abatch)):\n",
    "            paddedQbatch=np.array(apply_padding(Qbatch,questionwordsIDs))\n",
    "            paddedAbatch=np.array(apply_padding(Abatch,answerwordsIDs))\n",
    "            yield  paddedQbatch,paddedAbatch\n",
    "# Notice that yield is inside the loop\n",
    "# Return sends a specified value back to its caller whereas Yield can produce a sequence of values. \n",
    "# We should use yield when we want to iterate over a sequence, but don’t want to store the entire sequence in memory.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data (Q&A) into train/dev/test sets.\n",
    "\n",
    "train_val_split=int(len(SortclQues)*0.15)\n",
    "\n",
    "training_quest=SortclQues[train_val_split:]\n",
    "training_answ=SortclAns[train_val_split:]\n",
    "\n",
    "validation_quest=SortclQues[0:train_val_split]\n",
    "validation_answ=SortclAns[0:train_val_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training:\n",
    "\n",
    "batch_index_check_training_loss = 100\n",
    "batch_index_check_validation_loss = ((len(training_answ)) // batch_size // 2) - 1\n",
    "total_training_loss_error = 0\n",
    "list_validation_loss_error = []\n",
    "early_stopping_check = 0\n",
    "early_stopping_stop = 1000\n",
    "checkpoint = \"./chatbot_weights.ckpt\" \n",
    "session.run(tf.global_variables_initializer())\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_quest, training_answ, batch_size)):\n",
    "        starting_time = time.time()\n",
    "        _, batch_training_loss_error = session.run([oprimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n",
    "                                                                                               targets: padded_answers_in_batch,\n",
    "                                                                                               lr: learning_rate,\n",
    "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                                               keep_prob: 0.5})\n",
    "        total_training_loss_error += batch_training_loss_error\n",
    "        ending_time = time.time()\n",
    "        batch_time = ending_time - starting_time\n",
    "        if batch_index % batch_index_check_training_loss == 0:\n",
    "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
    "                                                                                                                                       epochs,\n",
    "                                                                                                                                       batch_index,\n",
    "                                                                                                                                       len(training_quest) // batch_size,\n",
    "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
    "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
    "            total_training_loss_error = 0\n",
    "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
    "            total_validation_loss_error = 0\n",
    "            starting_time = time.time()\n",
    "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_quest, validation_answ, batch_size)):\n",
    "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
    "                                                                       targets: padded_answers_in_batch,\n",
    "                                                                       lr: learning_rate,\n",
    "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                       keep_prob: 1})\n",
    "                total_validation_loss_error += batch_validation_loss_error\n",
    "            ending_time = time.time()\n",
    "            batch_time = ending_time - starting_time\n",
    "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
    "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
    "            #implementing learning rate decay.\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            # Now for early stopping:\n",
    "            list_validation_loss_error.append(average_validation_loss_error)\n",
    "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
    "                print('I speak better now!!') # Meaning we improved validation loss error. It is smaller than before.\n",
    "                early_stopping_check = 0\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session, checkpoint) #we defined checkpoint before\n",
    "            else:\n",
    "                print(\"Sorry I do not speak better, I need to practice more.\")\n",
    "                early_stopping_check += 1\n",
    "                if early_stopping_check == early_stopping_stop:\n",
    "                    break\n",
    "    if early_stopping_check == early_stopping_stop:\n",
    "        print(\"Sorry! I can't speak better anymore, this is my limit!\")\n",
    "        break\n",
    "print(\"Game Over\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is going to take a little while based on how strong your computer is, so if it takes to long we might want to use a lower number of epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
